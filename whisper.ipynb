{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch\n",
    "# pip install git+https://github.com/openai/whisper.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:07<00:00, 19.8MiB/s]\n",
      "c:\\Users\\virgi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here's how I built an AI that knows everything about Formula One. So first I need to collect a few URLs containing up-to-date information about Formula One. Next I'm going to write a function to loop through each of the URLs and scrape the contents of each web page. And then I'm going to split the content into digestible chunks. You'll see why later. Now that all of the content has been scraped I need to convert the chunks of text into data that can be stored in a vector database. Because this is what will allow us to search or update records to find the answer for something. To do this I'm using one of OpenAI's embedding models to convert the sequences of text into a raise of numbers. But you don't have to use OpenAI, there are a bunch of three alternatives. With all of this data stored in the database we're now going to write some code to allow us to ask some questions about Formula One. When a question is submitted we're going to first generate an embedding of the message and like we did previously. Now using the vector database we can search for chunks of information that closely matched the wording of the question. For example a query that contains who was the 2024 World Champion was searched for chunks which contained 2024 World Champion and anything similar. So when the database returns similar documents we can then use this information as context to generate a response using any large language model. Now let's give it a run to try it out. And boom now you can create AI applications with up to date information. If you want to break into tech follow for more.\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(\"audio.mp3\")\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('script.txt','w') as fichier :\n",
    "    for i in result[\"text\"].split('.'):\n",
    "        fichier.write(i+'.\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
